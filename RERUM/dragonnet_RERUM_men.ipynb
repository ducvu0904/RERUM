{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import random\n",
        "import os\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "def seed_everything(seed):\n",
        "    \"\"\"\n",
        "    \"\"\"\n",
        "    # 1. Python & Numpy\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    \n",
        "    # 2. PyTorch (CPU & GPU)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    \n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    \n",
        "    print(f\"üîí Locked Random Seed: {seed}\")\n",
        "\n",
        "# --- G·ªåI H√ÄM ---\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#import\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "---------------------------\n",
            "null count:\n",
            "recency            0\n",
            "history_segment    0\n",
            "history            0\n",
            "mens               0\n",
            "womens             0\n",
            "zip_code           0\n",
            "newbie             0\n",
            "channel            0\n",
            "visit              0\n",
            "conversion         0\n",
            "spend              0\n",
            "treatment          0\n",
            "dtype: int64\n",
            "---------------------------\n",
            "recency              int64\n",
            "history_segment     object\n",
            "history            float64\n",
            "mens                 int64\n",
            "womens               int64\n",
            "zip_code            object\n",
            "newbie               int64\n",
            "channel             object\n",
            "visit                int64\n",
            "conversion           int64\n",
            "spend              float64\n",
            "treatment            int64\n",
            "dtype: object\n",
            "---------------------------\n",
            "labels:\n",
            "['recency', 'history_segment', 'history', 'mens', 'womens', 'zip_code', 'newbie', 'channel', 'visit', 'conversion', 'spend', 'treatment']\n",
            "---------------------------\n",
            "data shape:\n",
            "(42613, 12)\n"
          ]
        }
      ],
      "source": [
        "#load data\n",
        "df_men = pd.read_csv(r\"C:\\Users\\Lenovo\\Documents\\Neu 2025-2026\\Lab\\Hillstrom-Men.csv\")\n",
        "df_men = df_men.drop(columns=\"Unnamed: 0\")\n",
        "print (\"---------------------------\")\n",
        "print (\"null count:\")\n",
        "print (df_men.isnull().sum())\n",
        "print (\"---------------------------\")\n",
        "print(df_men.dtypes)\n",
        "print (\"---------------------------\")\n",
        "print (\"labels:\")\n",
        "print(df_men.columns.tolist())\n",
        "print (\"---------------------------\")\n",
        "print(\"data shape:\")\n",
        "print(df_men.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[[ 6.36764744e-01  9.81582400e-01  9.35637990e-01 ...  0.00000000e+00\n",
            "   1.00000000e+00  0.00000000e+00]\n",
            " [ 6.36764744e-01 -9.60390610e-01 -8.34987151e-01 ...  0.00000000e+00\n",
            "   1.00000000e+00  0.00000000e+00]\n",
            " [ 1.49199505e+00 -3.13066274e-01 -4.45449620e-01 ...  1.00000000e+00\n",
            "   1.00000000e+00  0.00000000e+00]\n",
            " ...\n",
            " [-1.07369588e+00  3.34258063e-01 -6.37815337e-02 ...  1.00000000e+00\n",
            "   0.00000000e+00  0.00000000e+00]\n",
            " [-1.35877265e+00  3.34258063e-01 -8.25973163e-04 ...  0.00000000e+00\n",
            "   1.00000000e+00  0.00000000e+00]\n",
            " [-5.03542338e-01 -9.60390610e-01 -7.75966313e-01 ...  1.00000000e+00\n",
            "   1.00000000e+00  0.00000000e+00]]\n"
          ]
        }
      ],
      "source": [
        "#Hillstrom-men\n",
        "#split num and cate\n",
        "\n",
        "cate_cols = ['zip_code', 'channel']\n",
        "df_men[\"history_segment\"] =df_men[\"history_segment\"].map({\n",
        "    \"1) $0 - $100\": '1', \n",
        "    \"2) $100 - $200\": 2, \n",
        "    \"3) $200 - $350\": \"3\",\n",
        "    \"4) $350 - $500\": \"4\",\n",
        "    \"5) $500 - $750\": \"5\",\n",
        "    \"6) $750 - $1,000\": \"6\",\n",
        "    \"7) $1,000 +\": \"7\"                         \n",
        "})\n",
        "num_cols = ['recency', 'history', 'history_segment']\n",
        "\n",
        "#split x y t\n",
        "y_men = df_men[\"spend\"]\n",
        "t_men = df_men[\"treatment\"]\n",
        "x_men = df_men.drop(columns=[\"spend\", \"treatment\", \"visit\", \"conversion\"])\n",
        "\n",
        "x_men_encode = pd.get_dummies(x_men, columns=cate_cols, drop_first=True)\n",
        "x_men_encode = x_men_encode.astype(int)\n",
        "#train test split\n",
        "x_men_train, x_men_test_val,t_men_train, t_men_test_val, y_men_train, y_men_test_val = train_test_split(x_men_encode,t_men.values, y_men.values, test_size=0.4, random_state=42, stratify=t_men)\n",
        "x_men_val, x_men_test, t_men_val, t_men_test, y_men_val, y_men_test = train_test_split(x_men_test_val, t_men_test_val, y_men_test_val, test_size= 0.75, random_state=42, stratify=t_men_test_val)\n",
        "\n",
        "#scale\n",
        "scaler = StandardScaler()\n",
        "x_men_train[num_cols] = scaler.fit_transform(x_men_train[num_cols])\n",
        "x_men_val[num_cols] = scaler.transform(x_men_val[num_cols])\n",
        "x_men_test [num_cols ]= scaler.transform(x_men_test[num_cols])\n",
        "\n",
        "x_men_train = x_men_train.values.astype(float)\n",
        "x_men_val = x_men_val.values.astype(float)\n",
        "x_men_test = x_men_test.values.astype(float)\n",
        "print (x_men_train)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-------------------------------------------------------------\n",
            "‚úÖCompleted tranform to tensor‚úÖ\n",
            "Shape of train: x=torch.Size([25567, 10]); y =torch.Size([25567, 1]); t=torch.Size([25567, 1])\n",
            "Shape of val: x=torch.Size([4261, 10]); y=torch.Size([4261, 1]); t=torch.Size([4261, 1])\n",
            "Shape of test: x=torch.Size([12785, 10]); y=torch.Size([12785, 1]); t=torch.Size([12785, 1])\n"
          ]
        }
      ],
      "source": [
        "#Transform to tensor\n",
        "def to_tensor(df):\n",
        "    return torch.tensor(df, dtype=torch.float32)\n",
        "\n",
        "x_men_train_t = to_tensor(x_men_train)\n",
        "x_men_val_t = to_tensor(x_men_val)\n",
        "x_men_test_t = to_tensor(x_men_test)\n",
        "\n",
        "y_men_train_t = to_tensor(y_men_train).unsqueeze(1)\n",
        "y_men_val_t = to_tensor(y_men_val).unsqueeze(1)\n",
        "y_men_test_t = to_tensor(y_men_test).unsqueeze(1)\n",
        "\n",
        "t_men_train_t = to_tensor(t_men_train.astype(float)).unsqueeze(1)\n",
        "t_men_val_t = to_tensor(t_men_val.astype(float)).unsqueeze(1)\n",
        "t_men_test_t = to_tensor(t_men_test.astype(float)).unsqueeze(1)\n",
        "\n",
        "#Data loader\n",
        "train_dataset = TensorDataset(x_men_train_t, t_men_train_t, y_men_train_t)\n",
        "val_dataset = TensorDataset(x_men_val_t, t_men_val_t, y_men_val_t)\n",
        "test_dataset = TensorDataset(x_men_test_t, t_men_test_t, y_men_test_t)\n",
        "\n",
        "batch_size = 26000\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, num_workers= 4, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size,num_workers= 4, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, num_workers= 4, shuffle=False)\n",
        "\n",
        "print (\"-------------------------------------------------------------\")\n",
        "print (\"‚úÖCompleted tranform to tensor‚úÖ\")\n",
        "print (f\"Shape of train: x={x_men_train_t.shape}; y ={y_men_train_t.shape}; t={t_men_train_t.shape}\")\n",
        "print (f\"Shape of val: x={x_men_val_t.shape}; y={y_men_val_t.shape}; t={t_men_val_t.shape}\")\n",
        "print (f\"Shape of test: x={x_men_test_t.shape}; y={y_men_test_t.shape}; t={t_men_test_t.shape}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Evaluation metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from metrics import auuc, auqc, lift, krcc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Build Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from dragonnet import Dragonnet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üìä Data Distribution Check:\n",
            "Y train: mean=1.0188, std=14.8554\n",
            "Y train zeros: 25342 / 25567 (99.1%)\n",
            "\n",
            "Treatment balance:\n",
            "  Train: 12784 treated, 12783 control\n",
            "  Test:  6392 treated, 6393 control\n"
          ]
        }
      ],
      "source": [
        "print(\"üìä Data Distribution Check:\")\n",
        "print(f\"Y train: mean={y_men_train.mean():.4f}, std={y_men_train.std():.4f}\")\n",
        "print(f\"Y train zeros: {(y_men_train == 0).sum()} / {len(y_men_train)} ({(y_men_train == 0).sum()/len(y_men_train)*100:.1f}%)\")\n",
        "print(f\"\\nTreatment balance:\")\n",
        "print(f\"  Train: {(t_men_train == 1).sum()} treated, {(t_men_train == 0).sum()} control\")\n",
        "print(f\"  Test:  {(t_men_test == 1).sum()} treated, {(t_men_test == 0).sum()} control\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîí Locked Random Seed: 5\n"
          ]
        }
      ],
      "source": [
        "seed_everything(6)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîÉüîÉüîÉBegin training DragonnetüîÉüîÉüîÉ\n",
            "Epoch 1 | Train Loss: 146312544256.0000 | VAL LOSS: 7253051392.0000\n",
            "Epoch 2 | Train Loss: 146231721984.0000 | VAL LOSS: 7250505216.0000\n",
            "Epoch 3 | Train Loss: 146192678912.0000 | VAL LOSS: 7248637440.0000\n",
            "Epoch 4 | Train Loss: 146167906304.0000 | VAL LOSS: 7247253504.0000\n",
            "Epoch 5 | Train Loss: 146143444992.0000 | VAL LOSS: 7246117888.0000\n",
            "Epoch 6 | Train Loss: 146112151552.0000 | VAL LOSS: 7245166080.0000\n",
            "Epoch 7 | Train Loss: 146077777920.0000 | VAL LOSS: 7244262912.0000\n",
            "Epoch 8 | Train Loss: 146043289600.0000 | VAL LOSS: 7243251712.0000\n",
            "Epoch 9 | Train Loss: 146007965696.0000 | VAL LOSS: 7241920000.0000\n",
            "Epoch 10 | Train Loss: 145968349184.0000 | VAL LOSS: 7240128000.0000\n",
            "Epoch 11 | Train Loss: 145920999424.0000 | VAL LOSS: 7237747200.0000\n",
            "Epoch 12 | Train Loss: 145863114752.0000 | VAL LOSS: 7234675712.0000\n",
            "Epoch 13 | Train Loss: 145792565248.0000 | VAL LOSS: 7230633984.0000\n",
            "Epoch 14 | Train Loss: 145702649856.0000 | VAL LOSS: 7224537088.0000\n",
            "Epoch 15 | Train Loss: 145566253056.0000 | VAL LOSS: 7216341504.0000\n",
            "Epoch 16 | Train Loss: 145382326272.0000 | VAL LOSS: 7205511680.0000\n",
            "Epoch 17 | Train Loss: 145144266752.0000 | VAL LOSS: 7191335936.0000\n",
            "Epoch 18 | Train Loss: 144866574336.0000 | VAL LOSS: 7178916352.0000\n",
            "Epoch 19 | Train Loss: 144760193024.0000 | VAL LOSS: 7174546944.0000\n",
            "Epoch 20 | Train Loss: 144747315200.0000 | VAL LOSS: 7172964864.0000\n",
            "Epoch 21 | Train Loss: 144531161088.0000 | VAL LOSS: 7178548224.0000\n",
            "Epoch 22 | Train Loss: 144521281536.0000 | VAL LOSS: 7183872512.0000\n",
            "Epoch 23 | Train Loss: 144610263040.0000 | VAL LOSS: 7177957376.0000\n",
            "Epoch 24 | Train Loss: 144527704064.0000 | VAL LOSS: 7168954880.0000\n",
            "Epoch 25 | Train Loss: 144489578496.0000 | VAL LOSS: 7166464512.0000\n",
            "Epoch 26 | Train Loss: 144538304512.0000 | VAL LOSS: 7167838208.0000\n",
            "Epoch 27 | Train Loss: 144426303488.0000 | VAL LOSS: 7172700160.0000\n",
            "Epoch 28 | Train Loss: 144389242880.0000 | VAL LOSS: 7176183808.0000\n",
            "Epoch 29 | Train Loss: 144401580032.0000 | VAL LOSS: 7175440896.0000\n",
            "Epoch 30 | Train Loss: 144379625472.0000 | VAL LOSS: 7171629568.0000\n",
            "Epoch 31 | Train Loss: 144337076224.0000 | VAL LOSS: 7167303680.0000\n",
            "Epoch 32 | Train Loss: 144321200128.0000 | VAL LOSS: 7164634624.0000\n",
            "Epoch 33 | Train Loss: 144332423168.0000 | VAL LOSS: 7164177920.0000\n",
            "Epoch 34 | Train Loss: 144312713216.0000 | VAL LOSS: 7165799424.0000\n",
            "Epoch 35 | Train Loss: 144269885440.0000 | VAL LOSS: 7168683008.0000\n",
            "Epoch 36 | Train Loss: 144253353984.0000 | VAL LOSS: 7170128384.0000\n",
            "Epoch 37 | Train Loss: 144247226368.0000 | VAL LOSS: 7167752704.0000\n",
            "Epoch 38 | Train Loss: 144211066880.0000 | VAL LOSS: 7162572800.0000\n",
            "Epoch 39 | Train Loss: 144171974656.0000 | VAL LOSS: 7158772224.0000\n",
            "Epoch 40 | Train Loss: 144170057728.0000 | VAL LOSS: 7159082496.0000\n",
            "Epoch 41 | Train Loss: 144129687552.0000 | VAL LOSS: 7162962944.0000\n",
            "Epoch 42 | Train Loss: 144100638720.0000 | VAL LOSS: 7163750912.0000\n",
            "Epoch 43 | Train Loss: 144074113024.0000 | VAL LOSS: 7159267328.0000\n",
            "Epoch 44 | Train Loss: 144014180352.0000 | VAL LOSS: 7156600832.0000\n",
            "Epoch 45 | Train Loss: 143979855872.0000 | VAL LOSS: 7158542848.0000\n",
            "Epoch 46 | Train Loss: 143904620544.0000 | VAL LOSS: 7162356224.0000\n",
            "Epoch 47 | Train Loss: 143845392384.0000 | VAL LOSS: 7161512448.0000\n",
            "Epoch 48 | Train Loss: 143764471808.0000 | VAL LOSS: 7157889024.0000\n",
            "Epoch 49 | Train Loss: 143678881792.0000 | VAL LOSS: 7157998592.0000\n",
            "Epoch 50 | Train Loss: 143588048896.0000 | VAL LOSS: 7162375680.0000\n",
            "Epoch 51 | Train Loss: 143502753792.0000 | VAL LOSS: 7163721728.0000\n",
            "Epoch 52 | Train Loss: 143428861952.0000 | VAL LOSS: 7161412096.0000\n",
            "Epoch 53 | Train Loss: 143354429440.0000 | VAL LOSS: 7162883072.0000\n",
            "Epoch 54 | Train Loss: 143290892288.0000 | VAL LOSS: 7165264896.0000\n",
            "‚èπÔ∏è Early stopped at epoch 54 because Val loss doesnt reduce.\n",
            "Complete training\n"
          ]
        }
      ],
      "source": [
        "dragonnet = Dragonnet(input_dim=x_men_train_t.shape[1], epochs=100,ranking_lambda=10.0, alpha = 1.0, beta =1.0, learning_rate=0.001)\n",
        "dragonnet.fit(train_loader, val_loader)\n",
        "\n",
        "print (\"Complete training\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "üìä Model Output Check:\n",
            "y0_pred: min=0.6358, max=0.6358, mean=0.6358\n",
            "y1_pred: min=3.5270, max=7.8734, mean=5.2306\n",
            "propensity_score:  min=0.0434, max=0.9951, mean=0.1809\n",
            "Uplift:  min=2.8912, max=7.2377, std=0.4598\n"
          ]
        }
      ],
      "source": [
        "# Sau khi train xong\n",
        "y0_pred, y1_pred, t_pred, _ = dragonnet.predict(x_men_test_t)  # Test 100 samples\n",
        "\n",
        "print(\"\\nüìä Model Output Check:\")\n",
        "print(f\"y0_pred: min={y0_pred.min():.4f}, max={y0_pred.max():.4f}, mean={y0_pred.mean():.4f}\")\n",
        "print(f\"y1_pred: min={y1_pred.min():.4f}, max={y1_pred.max():.4f}, mean={y1_pred.mean():.4f}\")\n",
        "print(f\"propensity_score:  min={t_pred.min():.4f}, max={t_pred.max():.4f}, mean={t_pred.mean():.4f}\")\n",
        "\n",
        "uplift = (y1_pred - y0_pred).numpy().flatten()\n",
        "print(f\"Uplift:  min={uplift.min():.4f}, max={uplift.max():.4f}, std={uplift.std():.4f}\")\n",
        "\n",
        "if uplift.std() < 0.01:\n",
        "    print(\"‚ö†Ô∏è WARNING: Model is predicting almost constant uplift!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluating baselineüîÉüîÉüîÉ\n"
          ]
        },
        {
          "ename": "TypeError",
          "evalue": "'numpy.float64' object is not callable",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      6\u001b[39m y_true = y_men_test_t.numpy().flatten()\n\u001b[32m      7\u001b[39m t_true = t_men_test_t.numpy().flatten()\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m auuc = \u001b[43mauuc\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43muplift_pred\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbins\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m100\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mplot\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m auqc = auqc(y_true, t_true, uplift_pred, bins=\u001b[32m100\u001b[39m, plot=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m     11\u001b[39m lift = lift(y_true, t_true, uplift_pred, h=\u001b[32m0.3\u001b[39m, bins=\u001b[32m100\u001b[39m)\n",
            "\u001b[31mTypeError\u001b[39m: 'numpy.float64' object is not callable"
          ]
        }
      ],
      "source": [
        "print (\"Evaluating baselineüîÉüîÉüîÉ\")\n",
        "y0_pred, y1_pred, _,_ = dragonnet.predict(x_men_test_t)\n",
        "\n",
        "uplift_pred = (y1_pred - y0_pred).numpy().flatten()\n",
        "\n",
        "y_true = y_men_test_t.numpy().flatten()\n",
        "t_true = t_men_test_t.numpy().flatten()\n",
        "\n",
        "auuc = auuc(y_true, t_true, uplift_pred, bins=100, plot=True)\n",
        "auqc = auqc(y_true, t_true, uplift_pred, bins=100, plot=True)\n",
        "lift = lift(y_true, t_true, uplift_pred, h=0.3, bins=100)\n",
        "krcc = krcc(y_true, t_true, uplift_pred, bins= 100)\n",
        "\n",
        "print (\"-\"*40)\n",
        "print (\"AUUC: \", auuc)\n",
        "print (\"AUQC: \", auqc)\n",
        "print (\"Lift: \", lift)\n",
        "print (\"KRCC: \", krcc)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
