{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import numpy as np\n",
        "import random\n",
        "\n",
        "def seed_everything(seed):\n",
        "    \"\"\"\n",
        "    Set random seed for reproducibility\n",
        "    \"\"\"\n",
        "    # 1. Python & Numpy\n",
        "    random.seed(seed)\n",
        "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
        "    np.random.seed(seed)\n",
        "    \n",
        "    # 2. PyTorch (CPU & GPU)\n",
        "    torch.manual_seed(seed)\n",
        "    torch.cuda.manual_seed(seed)\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    \n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    \n",
        "    print(f\"üîí Locked Random Seed: {seed}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "\n",
        "def seed_everything_random():\n",
        "    \"\"\"\n",
        "    T·∫°o random seed, set seed ƒë√≥, v√† return seed ƒë·ªÉ b·∫°n bi·∫øt\n",
        "    \"\"\"\n",
        "    # T·∫°o random seed\n",
        "    random_seed = random.randint(0, 999999)\n",
        "    \n",
        "    # Set seed\n",
        "    torch.manual_seed(random_seed)\n",
        "    torch.cuda.manual_seed(random_seed)\n",
        "    torch.cuda.manual_seed_all(random_seed)\n",
        "    np.random.seed(random_seed)\n",
        "    random.seed(random_seed)\n",
        "    \n",
        "    # ƒê·ªÉ reproducible\n",
        "    torch.backends.cudnn.deterministic = True\n",
        "    torch.backends.cudnn.benchmark = False\n",
        "    \n",
        "    return random_seed\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#import\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from preprocess import get_sampler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#load data\n",
        "df_men =pd.read_csv(r\"C:\\Users\\Lenovo\\Documents\\Neu 2025-2026\\Lab\\Hillstrom-Men.csv\")\n",
        "df_men = df_men.drop(columns=\"Unnamed: 0\")\n",
        "print (\"---------------------------\")\n",
        "print (\"null count:\")\n",
        "print (df_men.isnull().sum())\n",
        "print (\"---------------------------\")\n",
        "print(df_men.dtypes)\n",
        "print (\"---------------------------\")\n",
        "print (\"labels:\")\n",
        "print(df_men.columns.tolist())\n",
        "print (\"---------------------------\")\n",
        "print(\"data shape:\")\n",
        "print(df_men.shape)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Hillstrom-men\n",
        "#split num and cate\n",
        "cate_cols = ['zip_code', 'channel']\n",
        "df_men[\"history_segment\"] =df_men[\"history_segment\"].map({\n",
        "    \"1) $0 - $100\": '1', \n",
        "    \"2) $100 - $200\": \"2\", \n",
        "    \"3) $200 - $350\": \"3\",\n",
        "    \"4) $350 - $500\": \"4\",\n",
        "    \"5) $500 - $750\": \"5\",\n",
        "    \"6) $750 - $1,000\": \"6\",\n",
        "    \"7) $1,000 +\": \"7\"                         \n",
        "})\n",
        "num_cols = ['recency', 'history_segment']\n",
        "#split x y t\n",
        "y_men = df_men[\"spend\"]\n",
        "t_men = df_men[\"treatment\"]\n",
        "x_men = df_men.drop(columns=[\"spend\", \"treatment\", \"visit\", \"conversion\", 'history'])\n",
        "\n",
        "x_men_encode = pd.get_dummies(x_men, columns=cate_cols, drop_first=True)\n",
        "x_men_encode = x_men_encode.astype(float)\n",
        "#train test split\n",
        "x_men_train, x_men_test_val, t_men_train, t_men_test_val, y_men_train, y_men_test_val = train_test_split(x_men_encode,t_men.values, y_men.values, test_size=0.4, random_state=42, stratify=t_men)\n",
        "x_men_val, x_men_test, t_men_val, t_men_test, y_men_val, y_men_test = train_test_split(x_men_test_val, t_men_test_val, y_men_test_val, test_size= 0.75, random_state=42, stratify=t_men_test_val)\n",
        "\n",
        "#scale\n",
        "# scaler = StandardScaler()\n",
        "# x_men_train[num_cols]= scaler.fit_transform(x_men_train[num_cols])\n",
        "# # x_men_val[num_cols] = scaler.transform(x_men_val[num_cols])\n",
        "# x_men_test[num_cols] = scaler.transform(x_men_test[num_cols])\n",
        "\n",
        "x_men_train = x_men_train.values.astype(float)\n",
        "x_men_val = x_men_val.values.astype(float)\n",
        "x_men_test = x_men_test.values.astype(float)\n",
        "print (x_men_train[:10])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Transform to tensor\n",
        "def to_tensor(df):\n",
        "    return torch.tensor(df, dtype=torch.float32)\n",
        "\n",
        "x_men_train_t = to_tensor(x_men_train)\n",
        "x_men_val_t = to_tensor(x_men_val)\n",
        "x_men_test_t = to_tensor(x_men_test)\n",
        "\n",
        "y_men_train_t = to_tensor(y_men_train).unsqueeze(1)\n",
        "y_men_val_t = to_tensor(y_men_val).unsqueeze(1)\n",
        "y_men_test_t = to_tensor(y_men_test).unsqueeze(1)\n",
        "\n",
        "t_men_train_t = to_tensor(t_men_train.astype(float)).unsqueeze(1)\n",
        "t_men_val_t = to_tensor(t_men_val.astype(float)).unsqueeze(1)\n",
        "t_men_test_t = to_tensor(t_men_test.astype(float)).unsqueeze(1)\n",
        "\n",
        "\n",
        "#dual stream \n",
        "idx_t = (t_men_train==1)\n",
        "idx_c = (t_men_train==0)\n",
        "\n",
        "x_treat = x_men_train_t[idx_t]\n",
        "t_treat = t_men_train_t[idx_t]\n",
        "y_treat = y_men_train_t[idx_t]\n",
        "\n",
        "x_ctrl = x_men_train_t[idx_c]\n",
        "t_ctrl = t_men_train_t[idx_c]\n",
        "y_ctrl = y_men_train_t[idx_c]\n",
        "\n",
        "sampler_treat = get_sampler(y_treat, target_positive_ratio=0.2)\n",
        "sampler_control = get_sampler(y_ctrl, target_positive_ratio=0.2)\n",
        "#Data loader\n",
        "train_t_dataset = TensorDataset(x_men_train_t[idx_t], t_men_train_t[idx_t], y_men_train_t[idx_t])\n",
        "train_c_dataset = TensorDataset(x_men_train_t[idx_c], t_men_train_t[idx_c], y_men_train_t[idx_c])\n",
        "val_dataset = TensorDataset(x_men_val_t, t_men_val_t, y_men_val_t)\n",
        "test_dataset = TensorDataset(x_men_test_t, t_men_test_t, y_men_test_t)\n",
        "\n",
        "batch_size = 6400\n",
        "# train_t_loader = DataLoader(train_t_dataset, batch_size= batch_size//2, sampler = sampler_treat, shuffle=False)\n",
        "# train_c_loader = DataLoader(train_c_dataset, batch_size= batch_size//2, sampler= sampler_control, shuffle= False)\n",
        "train_t_loader = DataLoader(train_t_dataset, batch_size= batch_size//2, shuffle=True)\n",
        "train_c_loader = DataLoader(train_c_dataset, batch_size= batch_size//2, shuffle= True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "print (\"-------------------------------------------------------------\")\n",
        "print (\"‚úÖCompleted tranform to tensor‚úÖ\")\n",
        "print (f\"Shape of train: x={x_men_train_t.shape}; y ={y_men_train_t.shape}; t={t_men_train_t.shape}\")\n",
        "print (f\"Shape of val: x={x_men_val_t.shape}; y={y_men_val_t.shape}; t={t_men_val_t.shape}\")\n",
        "print (f\"Shape of test: x={x_men_test_t.shape}; y={y_men_test_t.shape}; t={t_men_test_t.shape}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Evaluation metrics"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from metrics import auuc, auqc, lift, krcc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Build Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from dragonnet import Dragonnet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"üìä Data Distribution Check:\")\n",
        "print(f\"Y train: mean={y_men_train.mean():.4f}, std={y_men_train.std():.4f}\")\n",
        "print(f\"Y train zeros: {(y_men_train == 0).sum()} / {len(y_men_train)} ({(y_men_train == 0).sum()/len(y_men_train)*100:.1f}%)\")\n",
        "print(f\"\\nTreatment balance:\")\n",
        "print(f\"  Train: {(t_men_train == 1).sum()} treated, {(t_men_train == 0).sum()} control\")\n",
        "print(f\"  Test:  {(t_men_test == 1).sum()} treated, {(t_men_test == 0).sum()} control\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# seed = seed_everything_random()\n",
        "# print(f\"Using seed: {seed}\")\n",
        "\n",
        "# print(f\"Experiment completed with seed: {seed}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "seed = 10\n",
        "seed_everything(seed)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# C√†i ƒë·∫∑t Optuna (ch·ªâ c·∫ßn ch·∫°y 1 l·∫ßn)\n",
        "!pip install optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import optuna\n",
        "from optuna.visualization import plot_optimization_history, plot_param_importances\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def objective(trial):\n",
        "    \"\"\"\n",
        "    Objective function cho Optuna ƒë·ªÉ t·ªëi ∆∞u theo validation loss\n",
        "    T·ªëi ∆∞u: learning_rate, weight_decay, alpha, beta\n",
        "    \"\"\"\n",
        "    # Suggest hyperparameters\n",
        "    lr = trial.suggest_float('learning_rate', 1e-5, 1e-2, log=True)\n",
        "    weight_decay = trial.suggest_float('weight_decay', 1e-6, 1e-2, log=True)\n",
        "    \n",
        "    # Th√™m alpha v√† beta v√†o t·ªëi ∆∞u v·ªõi range h·ª£p l√Ω\n",
        "    alpha = trial.suggest_float('alpha', 0.0, 2.0)\n",
        "    beta = trial.suggest_float('beta', 0.0, 2.0)\n",
        "    \n",
        "    # Set seed cho reproducibility\n",
        "    seed_everything(seed)\n",
        "    \n",
        "    # Kh·ªüi t·∫°o model v·ªõi hyperparameters ƒë∆∞·ª£c suggest\n",
        "    model = Dragonnet(\n",
        "        input_dim=x_men_train_t.shape[1],\n",
        "        epochs=30,  # Gi·∫£m epochs ƒë·ªÉ t·ªëi ∆∞u nhanh h∆°n\n",
        "        alpha=alpha,\n",
        "        beta=beta,\n",
        "        learning_rate=lr,\n",
        "        weight_decay=weight_decay\n",
        "    )\n",
        "    \n",
        "    # Train model\n",
        "    model.fit(train_t_loader, train_c_loader, val_loader)\n",
        "    \n",
        "    # Evaluate tr√™n validation set - TR·∫¢ V·ªÄ LOSS\n",
        "    val_loss = model.validate(val_loader)\n",
        "    \n",
        "    # Report intermediate value ƒë·ªÉ c√≥ th·ªÉ d·ª´ng s·ªõm c√°c trial kh√¥ng t·ªët\n",
        "    trial.report(val_loss, step=30)\n",
        "    \n",
        "    # Handle pruning: d·ª´ng trial n·∫øu kh√¥ng promising\n",
        "    if trial.should_prune():\n",
        "        raise optuna.TrialPruned()\n",
        "    \n",
        "    return val_loss\n",
        "\n",
        "print(\"‚úÖ Objective function ƒë√£ ƒë∆∞·ª£c ƒë·ªãnh nghƒ©a!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# T·∫°o Optuna study - MINIMIZE validation loss\n",
        "study = optuna.create_study(\n",
        "    direction='minimize',  # MINIMIZE validation loss\n",
        "    pruner=optuna.pruners.MedianPruner(n_startup_trials=5, n_warmup_steps=10),\n",
        "    study_name='dragonnet_full_optimization'\n",
        ")\n",
        "\n",
        "# Ch·∫°y optimization\n",
        "print(\"üöÄ B·∫Øt ƒë·∫ßu t·ªëi ∆∞u hyperparameters (LR, WD, Alpha, Beta)...\")\n",
        "print(\"=\" * 70)\n",
        "study.optimize(objective, n_trials=30, show_progress_bar=True)\n",
        "\n",
        "# In k·∫øt qu·∫£ t·ªët nh·∫•t\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"‚úÖ K·∫æT QU·∫¢ T·ªêI ∆ØU:\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"üèÜ Best Validation Loss: {study.best_value:.4f}\")\n",
        "print(f\"\\nüìä Best Hyperparameters:\")\n",
        "for param, value in study.best_params.items():\n",
        "    print(f\"  ‚Ä¢ {param}: {value:.6f}\")\n",
        "print(\"=\" * 70)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Visualize optimization history\n",
        "fig1 = plot_optimization_history(study)\n",
        "fig1.update_layout(\n",
        "    title=\"Optimization History (Validation Loss)\",\n",
        "    xaxis_title=\"Trial\",\n",
        "    yaxis_title=\"Validation Loss (Lower is Better)\",\n",
        "    height=500\n",
        ")\n",
        "fig1.show()\n",
        "\n",
        "# Visualize parameter importances\n",
        "fig2 = plot_param_importances(study)\n",
        "fig2.update_layout(\n",
        "    title=\"Hyperparameter Importance\",\n",
        "    height=500\n",
        ")\n",
        "fig2.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Train final model v·ªõi best hyperparameters\n",
        "best_lr = study.best_params['learning_rate']\n",
        "best_wd = study.best_params['weight_decay']\n",
        "best_alpha = study.best_params['alpha']\n",
        "best_beta = study.best_params['beta']\n",
        "\n",
        "seed_everything(seed)\n",
        "\n",
        "dragonnet_optimized = Dragonnet(\n",
        "    input_dim=x_men_train_t.shape[1],\n",
        "    epochs=100,  # Full epochs\n",
        "    alpha=best_alpha,\n",
        "    beta=best_beta,\n",
        "    learning_rate=best_lr,\n",
        "    weight_decay=best_wd\n",
        ")\n",
        "\n",
        "print(f\"üöÄ Training v·ªõi Best Hyperparameters:\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"  ‚Ä¢ Learning Rate:  {best_lr:.2e}\")\n",
        "print(f\"  ‚Ä¢ Weight Decay:   {best_wd:.2e}\")\n",
        "print(f\"  ‚Ä¢ Alpha:          {best_alpha:.4f}\")\n",
        "print(f\"  ‚Ä¢ Beta:           {best_beta:.4f}\")\n",
        "print(\"=\" * 70)\n",
        "\n",
        "dragonnet_optimized.fit(train_t_loader, train_c_loader, val_loader)\n",
        "print(\"\\n‚úÖ Complete training v·ªõi hyperparameters t·ªëi ∆∞u!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Evaluate model v·ªõi hyperparameters t·ªëi ∆∞u\n",
        "print(\"üìä Evaluating optimized model...\")\n",
        "y0_pred, y1_pred, _, _ = dragonnet_optimized.predict(x_men_test_t)\n",
        "\n",
        "uplift_pred = (y1_pred - y0_pred).numpy().flatten()\n",
        "y_true = y_men_test_t.numpy().flatten()\n",
        "t_true = t_men_test_t.numpy().flatten()\n",
        "\n",
        "auuc_opt = auuc(y_true, t_true, uplift_pred, bins=100, plot=True)\n",
        "auqc_opt = auqc(y_true, t_true, uplift_pred, bins=100, plot=True)\n",
        "lift_opt = lift(y_true, t_true, uplift_pred, h=0.3)\n",
        "krcc_opt = krcc(y_true, t_true, uplift_pred, bins=100)\n",
        "\n",
        "print(\"\\n\" + \"=\" * 70)\n",
        "print(\"üìà METRICS v·ªõi Optimized Hyperparameters:\")\n",
        "print(\"=\" * 70)\n",
        "print(f\"  AUUC: {auuc_opt:.4f}\")\n",
        "print(f\"  AUQC: {auqc_opt:.4f}\")\n",
        "print(f\"  Lift: {lift_opt:.4f}\")\n",
        "print(f\"  KRCC: {krcc_opt:.4f}\")\n",
        "print(f\"  Seed: {seed}\")\n",
        "print(\"=\" * 70)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
